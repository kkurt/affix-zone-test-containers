apiVersion: v1
kind: ConfigMap
metadata:
  name: customer-tran-generator-script
  namespace: {{ .Values.namespace }}
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "{{ mul 10 2 }}"
data:
  customer-tran-generator-script.py: |-
    #!/usr/bin/env python3
    import argparse
    import csv
    import multiprocessing as mp
    import os
    from pathlib import Path
    import uuid
    import random
    import time
    from datetime import datetime, timedelta

    def load_customer_ids(master_file):
        """
        Read and return a list of customer IDs from either:
         - a single CSV (with or without header), or
         - a directory of CSV parts (all files ending in .csv).
        """
        path = Path(master_file)
        if path.is_dir():
            csv_files = sorted(path.glob("*.csv"))
            if not csv_files:
                raise RuntimeError(f"No CSV files found in directory '{master_file}'")
        else:
            csv_files = [path]

        customer_ids = []
        for f in csv_files:
            try:
                with open(f, 'r', encoding='utf-8') as infile:
                    reader = csv.reader(infile)
                    header = next(reader, None)
                    if header and header[0].lower() == 'customer_id':
                        for row in reader:
                            if row:
                                customer_ids.append(row[0])
                    else:
                        infile.seek(0)
                        for row in reader:
                            if row:
                                customer_ids.append(row[0])
            except Exception as e:
                raise RuntimeError(f"Failed to load IDs from '{f}': {e}")
        if not customer_ids:
            raise RuntimeError(f"No valid customer IDs found in '{master_file}'")
        return customer_ids

    def generate_transactions(customer_ids, count, start_date, end_date):
        delta_seconds = (end_date - start_date).total_seconds()
        for _ in range(count):
            txn_id = str(uuid.uuid4())
            cust_id = random.choice(customer_ids)
            amount = round(random.uniform(10.0, 1000.0), 2)
            offset = random.uniform(0, delta_seconds)
            timestamp = (start_date + timedelta(seconds=offset)).isoformat()
            yield [txn_id, cust_id, str(amount), timestamp]

    def write_chunk(worker_id, customer_ids, count, start_date, end_date):
        start_time = time.time()


        dir_path.mkdir(parents=True, exist_ok=True)

        part_file = dir_path / f"{base_name}_part{worker_id}{ext}"
        interval = max(1, count // 10)
        with open(part_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL, lineterminator='\n')
            writer.writerow(['transaction_id', 'customer_id', 'amount', 'timestamp'])
            for i, record in enumerate(generate_transactions(customer_ids, count, start_date, end_date)):
                if i and i % interval == 0:
                    pct = (i / count) * 100
                    print(f"\rWorker {worker_id}: {pct:.2f}% complete", end="")
                writer.writerow(record)

        print(f"\rWorker {worker_id}: 100.00% complete in {time.time() - start_time:.2f}s")
        return str(part_file)

    if __name__ == "__main__":
        parser = argparse.ArgumentParser(
            description="Generate transactions CSV in parallel, linking to existing customer IDs."
        )
        parser.add_argument("output_file", help="Path to output transactions CSV file (will become a directory)")
        parser.add_argument("count", type=lambda x: int(float(x)), help="Total number of transactions to generate")
        parser.add_argument(
            "--master", required=True,
            help="Path to master customer CSV file or directory of CSV parts"
        )
        parser.add_argument("--workers", type=int, default=1, help="Number of parallel workers")
        parser.add_argument(
            "--start-date",
            type=lambda s: datetime.fromisoformat(s),
            default=datetime.now() - timedelta(days=365),
            help="Start of transaction timestamp range (ISO format)"
        )
        parser.add_argument(
            "--end-date",
            type=lambda s: datetime.fromisoformat(s),
            default=datetime.now(),
            help="End of transaction timestamp range (ISO format)"
        )

        args = parser.parse_args()
        try:
            # load IDs from either single CSV or directory of parts
            customers = load_customer_ids(args.master)
            print(f"Info: Loaded {len(customers)} customer IDs from {args.master}")

            total   = args.count
            workers = max(1, min(args.workers, mp.cpu_count()))
            chunk   = total // workers
            counts  = [chunk + (1 if i < total % workers else 0) for i in range(workers)]

            out_prefix = args.output_file

            out_path  = Path(out_prefix)
            base_name = out_path.stem
            ext       = out_path.suffix
            dir_path  = out_path.parent / out_path.name

            with mp.Pool(processes=workers) as pool:
                parts = pool.starmap(
                    write_chunk,
                    [(i, customers, counts[i], args.start_date, args.end_date)
                     for i in range(workers)]
                )

            # no merge: all parts live under the output directory
            print(f"Info: Successfully generated {total} transactions into {Path(out_prefix).with_name(Path(out_prefix).stem)}")
        except Exception as e:
            print(f"Error: {e}")
            exit(1)
