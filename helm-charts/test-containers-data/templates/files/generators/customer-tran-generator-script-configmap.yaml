apiVersion: v1
kind: ConfigMap
metadata:
  name: customer-tran-generator-script
  namespace: {{ .Values.namespace }}
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "{{ mul 10 2 }}"
data:
  customer-tran-generator-script.py: |-
    #!/usr/bin/env python3
    import argparse
    import csv
    import multiprocessing as mp
    import os
    import uuid
    import random
    import time
    from datetime import datetime, timedelta

    def load_customer_ids(master_file):
        with open(master_file, newline="") as f:
            return [row[0] for row in csv.reader(f) if row[0] != 'id']

    def generate_transactions(customer_ids, count, start_date, end_date):
        for _ in range(count):
            yield [
                str(uuid.uuid4()),
                random.choice(customer_ids),
                (start_date + (end_date - start_date) * random.random()).isoformat(),
                round(random.uniform(10.0, 1000.0), 2)
            ]

    def write_chunk(worker_id, customer_ids, count, start_date, end_date, out_prefix):
        start = time.time()
        interval = max(1, count // 10)
        temp_file = f"{out_prefix}.part{worker_id}"
        with open(temp_file, "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["transaction_id", "customer_id", "date", "amount"])
            for i, row in enumerate(generate_transactions(customer_ids, count, start_date, end_date)):
                writer.writerow(row)
                if (i + 1) % interval == 0 or (i + 1) == count:
                    elapsed = time.time() - start
                    percent = (i + 1) * 100 / count
                    eta = elapsed * (count / (i + 1) - 1)
                    print(f"[Tran] Worker {worker_id}: {i+1}/{count} ({percent:.1f}%) elapsed {elapsed:.1f}s eta {eta:.1f}s")
        return temp_file

    def merge_files(temp_files, final_file):
        with open(final_file, "w", newline="") as fout:
            first = True
            for temp in temp_files:
                with open(temp, "r") as fin:
                    for line in fin:
                        if first:
                            fout.write(line)
                        else:
                            if not line.startswith("transaction_id,"):
                                fout.write(line)
                os.remove(temp)
                first = False
        print(f"[Tran] Merged {len(temp_files)} files into {final_file}")

    if __name__ == "__main__":
        parser = argparse.ArgumentParser(description="Generate transactions CSV in parallel with progress.")
        parser.add_argument("output_file", help="Path to output CSV file")
        parser.add_argument("count", type=lambda x: int(float(x)), help="Total number of records to generate")
        parser.add_argument("--master", required=True, help="Path to master customer CSV")
        parser.add_argument("--workers", type=int, default=1, help="Number of parallel workers")
        parser.add_argument("--start-date", type=lambda s: datetime.fromisoformat(s), default=(datetime.now()-timedelta(days=365)), help="Start of date range")
        parser.add_argument("--end-date", type=lambda s: datetime.fromisoformat(s), default=datetime.now(), help="End of date range")
        args = parser.parse_args()

        customers = load_customer_ids(args.master)
        total = args.count
        workers = args.workers
        chunk = total // workers
        counts = [chunk + (1 if i < total % workers else 0) for i in range(workers)]

        out_prefix = args.output_file
        with mp.Pool(processes=workers) as pool:
            parts = pool.starmap(
                write_chunk,
                [(i, customers, counts[i], args.start_date, args.end_date, out_prefix) for i in range(workers)]
            )

        merge_files(parts, out_prefix)
        print(f"Info: Successfully generated {total} transactions to {out_prefix}")