apiVersion: v1
kind: ConfigMap
metadata:
  name: customer-generator-script
  namespace: {{ .Values.namespace }}
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "{{ mul 10 1 }}"
data:
  customer-generator-script.py: |-
    #!/usr/bin/env python3
    import argparse
    import csv
    import multiprocessing as mp
    import os
    import uuid
    import time
    from faker import Faker

    def generate_record(_):
        faker = Faker()
        return [
            str(uuid.uuid4()),
            faker.name(),
            faker.email(),
            faker.address().replace("\n", " "),
            faker.date_between(start_date='-5y', end_date='today').isoformat()
        ]

    def write_chunk(worker_id, count, out_prefix):
        start = time.time()
        interval = max(1, count // 10)
        temp_file = f"{out_prefix}.part{worker_id}"
        with open(temp_file, "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["id", "name", "email", "address", "signup_date"])
            for i in range(count):
                writer.writerow(generate_record(i))
                if (i + 1) % interval == 0 or (i + 1) == count:
                    elapsed = time.time() - start
                    percent = (i + 1) * 100 / count
                    eta = elapsed * (count / (i + 1) - 1)
                    print(f"[Customer] Worker {worker_id}: {i+1}/{count} ({percent:.1f}%) elapsed {elapsed:.1f}s eta {eta:.1f}s")
        return temp_file

    def merge_files(temp_files, final_file):
        with open(final_file, "w", newline="") as fout:
            first = True
            for temp in temp_files:
                with open(temp, "r") as fin:
                    for line in fin:
                        if first:
                            fout.write(line)
                        else:
                            if not line.startswith("id,"):
                                fout.write(line)
                os.remove(temp)
                first = False
        print(f"[Customer] Merged {len(temp_files)} files into {final_file}")

    if __name__ == "__main__":
        parser = argparse.ArgumentParser(description="Generate customers CSV in parallel with progress.")
        parser.add_argument("output_file", help="Path to output CSV file")
        parser.add_argument("count", type=lambda x: int(float(x)), help="Total number of records to generate")
        parser.add_argument("--workers", type=int, default=1, help="Number of parallel workers")
        args = parser.parse_args()

        total = args.count
        workers = args.workers
        chunk = total // workers
        counts = [chunk + (1 if i < total % workers else 0) for i in range(workers)]

        out_prefix = args.output_file
        with mp.Pool(processes=workers) as pool:
            parts = pool.starmap(write_chunk, [(i, counts[i], out_prefix) for i in range(workers)])

        merge_files(parts, out_prefix)
        print(f"Info: Successfully generated {total} customers to {out_prefix}")